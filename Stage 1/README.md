## PORJECT 1 (MICROBES): WHOLE GENOME SEQUENCING ANALYSIS OF THE SOUTH AFRICAN POLONY OUTBREAK
### INTRODUCTION
South Africa faced a serious public health problem in early 2017 due to a foodborne infection traced to polony, a processed cold meats. To confirm the identity and resistance profile of the infectious organism, whole genome sequencing (WGS) analysis becomes highly indispensable. Hence, this current project analyses WGS data from 100 bacterial isolates collected during the 2017â€“2018 South African outbreak, to:  
- Confirm the identity of the organism;
- Determine the organism's antimicrobial resistance (AMR) profile.
- Detect the presence of any toxin accelerating death rate.
- Suggest possible antibiotics or treatment options for managing the infection.

### METHOD
#### 1a. Downloading the datasets
The script containing the datasets was provided at: `https://raw.githubusercontent.com/HackBio-Internship/2025_project_collection/refs/heads/main/SA_Polony_100_download.sh`. 
The script (`SA_Polony_100_download.sh`) and the datasets were downloaded and saved properly using the following commands:
```bash
wget https://raw.githubusercontent.com/HackBio-Internship/2025_project_collection/refs/heads/main/SA_Polony_100_download.sh   # Downloads the script provided.

bash SA_Polony_100_download.sh                   # Downloads the data from the script provided.
ls 						                         # Confirms downloaded files.
mkdir raw_data                                   # Creates a new folder for the data.
mv *.fastg.gz raw_data/                          # Moves all the data (ending with the '.fastg.gz' suffix) into to 'raw_data' folder. 

ls -l raw_data | grep ^- | wc -l                 # Confirms the presence and numbers of the files in the 'raw_data' folder.
# ls -l list the files with their details; grep ^- filters regular files only, without folder; wc -l counts number lines, which is equivalent to the number of files in the folder.
```

#### 1b. Downsizing to 50 samples
The 100 samples were downsized to 50, using the following commands:
```bash
mkdir selected_samples                            # Creates a new folder for the randomly selceted data.

for base_name in $(ls raw_data/*_1.fastq.gz | sed 's/_1.fastq.gz//' | shuf -n 50); do   
    cp "${base_name}_1.fastq.gz" selected_samples/                                      
    cp "${base_name}_2.fastq.gz" selected_samples/                                      
done

# ls raw_data/*_1.fastq.gz lists only all forward read files, to forestall duplications;
# sed 's/_1.fastq.gz//' removes the _1.fastq.gz suffix to obtain only the base names; shuf -n 50 randomly selects 50 unique base names.
# The for loop then iterates over each of the selected base name, and
# cp "${base_name}_1.fastq.gz" sample_data/ copies the corresponding forward read of the randomly selected sample into the sample_data folder, while
# cp "${base_name}_2.fastq.gz" sample_data/ copies the corresponding reverse read into the folder.

ls -l selected_samples | grep ^- | wc -l          # Confirms the presence and numbers of the files in the 'selected_samples' folder.
```

#### 2. Quality control with `fastqc`
It is important to check the quality of the data. The quality control was perfomed as follows:
```bash
mkdir qc                                                           # Creates a new folder named 'qc' for the QC reports/outputs.

for sample in selected_samples/*_1.fastq.gz selected_samples/*_2.fastq.gz; do
    fastqc "$sample" -o qc/
done                                                               # The command runs FastQC on all forward and reverse reads of the selected samples and outputs the QC reports to the "qc" folder.

ls -l qc | grep ^- | wc -l                                         # Verifies the number of files in the `qc` folder.

mkdir multiqc_reports                                              # Creates a new folder named 'multiqc_reports' for the aggregated QC reports.

multiqc qc/ -o multiqc_reports/                                    # Aggregates the QC reports and outputs them to the 'multiqc_reports' folder. 
```
The ```html``` report generated by multiqc was downloaded to assess the quality profile of the samples.
The multiqc aggregated the QC reports generated for the forward and reverse reads for each of the 50 randomly-selected samples, now totally 100 samples. This accounted for the subsequent mention of 100 samples, particularly, in this section of the report. 
A large proportion of the selected samples passed the quality assessments. For instance, all 100 samples passed the `Per Sequence Quality Scores`, `Per Base N Content`, and `Adapter Content` assessments, while the `Sequence Quality Histograms` showed that 89 of the 100 samples passed, 4 had warnings, and 7 failed, indicating acceptable quality for most of the sample. 
However, there were few issues of concern in the quality assessment. For example, all 100 samples failed the `Per Base Sequence Content` assessment, while the `Per Sequence GC Content` assessment showed that 98 of the 100 samples had warnings and 2 failed this assessment. Also, the quality assessment indicated some level of duplications, sequence length variability, and overrepresented sequences. 
These suggest potential problems with bais, contamination at specific base positions, library preparations, or sequencing, which may impair the reliability of downstream analyses. Therefore, the samples need trimming and filtering preprocessing to address these quality issues. 
The sample preprocessing steps were performed as shown in the next section. 

#### 3. Trimming and filtering with `fastp`
The fastp was used to remove poor-quality bases, short reads or adapter contents, using the following commands:
```bash
mkdir trimmed_samples                                            # Creates a new folder named 'trimmed_samples' for the fastp reports.
mkdir fastp_reports                                              # Creates a new folder named 'fastp_reports' for the html and json outputs from fastp.
nano trim.sh                                                     # Write a script named 'trim.sh' containing the codes that execute fastp.
```
The command in the `trim.sh` script includes:
```bash
#!/bin/bash

for sample in $(ls selected_samples/*_1.fastq.gz | sed 's|.*/||; s/_1.fastq.gz//'); do
    fastp -i "selected_samples/${sample}_1.fastq.gz" \
          -I "selected_samples/${sample}_2.fastq.gz" \
          -o "trimmed_samples/${sample}_1_trimmed.fastq.gz" \
          -O "trimmed_samples/${sample}_2_trimmed.fastq.gz" \
          --html "fastp_reports/${sample}_fastp.html" \
          --json "fastp_reports/${sample}_fastp.json" \
          -q 20 -u 10 -l 50 --trim_front1 5 --trim_front2 5 \
          --cut_mean_quality 20 \
          --cut_front \
          --cut_tail \
          --cut_right \
          --detect_adapter_for_pe
done                                                    

# s|.*/|| leaves only the file name by removing the directory path, and s/_1.fastq.gz// removes the suffix _1.fastq.gz.
# -i is the first input file (forward read); -I is the second input file (reverse read); 
# -o is the output forward read file (after processing); -O is the output reverse read file (after processing).
# --html generats an HTML report of the fastp process into the specified folder.
# -q 20 function sets the the minimum quality score at 20 (trims off bases with a quality score below 20).
# -u 10 filters out reads with a low quality score.
# -l 50 retains only reads longer than 50 bases (discards reads shorter than 50 bases).
# --trim_front1 5 and --trim_front2 5 trims off the first 5 bases from the forward and reverse input files, respectively.
# --cut_mean_quality 20 trims reads according to the average quality score (maintains overall read quality instead of mere individual base quality).
# --detect_adapter_for_pe automatically detects and removes adapter sequences.
```
Running the `fastp` command:
```bash
bash trim.sh                                                   # Performs fastp.
```
Confirming outputs:
```bash
ls -l trimmed_samples | grep ^- | wc -l                        # Confirms the number of files in the 'trimmed_samples' folder.
ls -l fastp_reports | grep ^- | wc -l                          # Confirms the number of files in the 'fastp_reports' folder.
```
The `fastp` reports were aggregated with `multiqc` as follows:
```bash
mkdir new_multiqc_reports                                      # Creates a new folder named 'new_multiqc_reports' for the multiqc reports.
multiqc fastp_reports/ -o new_multiqc_reports/                 # Aggregates the fastp reports and outputs them in the new_multiqc_reports' folder.
```
The aggregated fastp reports were thereafter downloaded and assessed to confirm improved quality.

#### 4. De novo assembly with `spades.py`
De novo genome assembly was perform on trimmed reads using SPAdes genome assembler `spades.py` as follow:
```bash
mkdir assembly                                                 # Creates a new folder named 'assembly'.
nano assembler.sh                                              # Writes a bash script for the assembly command as shown below:
```
The command in the `assembler.sh` script:
```bash
#!/bin/bash

for sample in $(ls trimmed_samples/*_1_trimmed.fastq.gz | sed 's|.*/||; s/_1_trimmed.fastq.gz//'); do
    spades.py -1 "trimmed_samples/${sample}_1_trimmed.fastq.gz" \
              -2 "trimmed_samples/${sample}_2_trimmed.fastq.gz" \
              -o "assembly/${sample}_spades" \
	          --threads 4 \
              --phred-offset 33
done

# The ls trimmed_samples/*_1_trimmed.fastq.gz lists all the files with forward read in the trimmed_samples folder, while 
# The sed 's|.*/||; s/_1_trimmed.fastq.gz//' function removes the path and the suffix from the file name to extract the sample name.
# spades.py -1 ... -2 ... -o ... function runs SPAdes for each sample, specifies the paths to the forward and reverse trimmed reads and sets the output folder for each sampleâ€™s assembly.
# --threads 4 uses 4 threads for the assembly process to increase the speed and save runtime a bit.
# --phred-offset 33 enables SPAdes to correctly interpret the quality scores in the fastq files for Phred+33 quality score encoding contained in the samples.
```
Running the `spades.py` command: 
```bash
bash assembler.sh                                                # Performs de novo genome assembly with `Spades.py`.
```
The genome was successfully assembled, and the quality of the assembly was assessed with `quast.py` in the next section.
#### 5. Checking assembly quality with `quast.py`
```bash
nano quast.sh                                                    # Writes the script containing the next set of commands.
```

```bash
#!/bin/bash

mkdir quast_reports                                              # Creates output directory

for sample_dir in assembly/*/; do                                # Loops through each sample directory within assembly folder.
    sample=$(basename "$sample_dir")                             # Extracts the base name (sample name) of the sample directory.
    contigs_file="${sample_dir}contigs.fasta"                    # Constructs the full path to the contigs.fasta file.

    if [[ -f "$contigs_file" ]]; then                            # Checks for the presence of the contigs.fasta file before running 'quast'.
        quast.py -o "quast_reports/${sample}_quast" "$contigs_file"
    else
        echo "Warning: File not found for sample ${sample}: $contigs_file"
    fi
done
```
```bash
bash quast.sh                                                     # Runs quast. 
```
```bash
ls -l quast_reports | grep ^d | wc -l                             # Confirm the directories in the `quast_reports` folder.
```
To be able to assess the `quast` reports at a glance for all the samples, combined `quast` reports were aggregated as follows:
```bash
nano quast_combined.sh                                            # Writes the script containing the next set of commands.
```
```bash
#!/bin/bash

mkdir combined_quast_reports                                      # Makes an output directory.

declare -a samples=()                                             # Defines an array to hold the paths to the contigs.fasta files.

for sample_dir in assembly/*/; do                                 # Populates the array with paths to each contigs.fasta file.
    samples+=("${sample_dir}contigs.fasta")
done

quast.py -o combined_quast_reports "${samples[@]}"                 # Runs quast on all samples at a go, using the array.
```
```bash
bash quast_combined.sh                                               # Runs quast.
```
#### 6. Running `blast` on a selected sample to identity the organism 
To conserve time and other resources, BLAST was performed on a selected sample to confirm the identity of the organism. The sample was selected based on the best N50 value, which directly relates to other quality metrics being in good shape.
```bash
cd quast_reports                                                       # Navigates to the `quast` output directory
```
```bash                                                      
nano select_sample.sh                                                  # Writes the script for sample selection.
```
```bash
#!/bin/bash

best_sample=$(for dir in */; do                                         # Loops through each directory in the current folder (quast_reports).
    n50_value=$(grep "N50" "$dir/report.txt" | awk '{print $NF}');      # Extracts N50 value from the report.txt file and stores it in the variable n50_value.
    echo "$n50_value $dir";                                             # Prints the N50 value along with the directory name.
done | sort -n | tail -n 1 | awk '{print $2}')                          # Sort the output numerically and and selects the last entry (highest N50).
echo "Selected sample for BLAST based on N50: $best_sample"             # Extracts just the directory name.
```
The sample with the base name `SRR27013333` was selected.
```bash
cd ../                                                                  # Returns to the prevoius working directory.
```
```bash
nano blast.sh                                                           # Writes the script for BLAST.
```
```bash
#!/bin/bash

output_folder="blast_report"                                            # Defines output directory name.

mkdir -p "$output_folder"                                               # Creates the output directory.

spades_output="assembly"                                                # Defines the path to the spades.py output directory.

contig_file="$spades_output/SRR27013333_Genome_Sequencing_of_Listeria_monocytogenes_SA_outbreak_2017_spades/contigs.fasta"  # Defines the path to the contig file.

if [[ ! -f "$contig_file" ]]; then                                      # Confirms the existence of the contig file.
    echo "Contig file NOT FOUND!: $contig_file"
    exit 1
else
    echo "Contig file EXISTS!: $contig_file"
fi
# Netx steps:
# Extract the first 100 contigs (i.e., extract the first 202 lines from the fasta file, since each contig consists of 2 lines: header + sequence).
head -n 202 "$contig_file" | awk 'NR % 2 == 1 {print; getline; print}' > extracted_contigs.fasta

echo "Contigs extracted SUCCESSFULLY!"

# Run BLAST
echo "Running BLAST to confirm organism's identity..."

blastn -query extracted_contigs.fasta -db nt -out "$output_folder/blast_results.tsv" \
       -outfmt "6 std stitle" \
       -evalue 1e-5 -remote -max_target_seqs 10

echo "BLAST run successfully!"

# Confirm the identification of Listeria monocytogenes
if grep -q -i "listeria" "$output_folder/blast_results.tsv"; then
    echo "SUCCESS!: BLAST succesfully identified Listeria monocytogenes!"
else
    echo "FAIL!: BLAST could not indentify Listeria monocytogenes."
fi
```
```bash
bash blast.sh                                                            # Performs BLAST.
```
#### 7. Detecting Antimicrobial Resistance (AMR) genes with `Abricate`
The AMR genes in the identified causative organism was identified and summarized as follows:
##### 7a. Detecting AMR Genes
```bash
nano amr_detection.sh                                                   # Writes the script for AMR genes detection.
```
```bash
#!/bin/bash

# Define variables
assembly_dir="assembly"                                                 # Folder containing assembled genomes.
output_dir="amr_results"                                                # Output folder.
amr_db="card"                                                           # Database (Comprehensive Antibiotic Resistance Database) to use for Abricate.

# Next steps:
# Create output folder
mkdir -p $output_dir

# Process the contigs.fasta file of each sample and run Abricate to detect AMR genes
for sample_dir in $assembly_dir/*; do
    contigs_file="$sample_dir/contigs.fasta"
    if [[ -f "$contigs_file" ]]; then
        abricate --db $amr_db --quiet "$contigs_file" > "$output_dir/$(basename $sample_dir)_amr_results.tsv"
    else
        echo "Warning: contigs.fasta not found in $sample_dir"
    fi
done

echo "AMR analysis completed. Results saved in $output_dir."
```
```bash
bash amr_detection.sh                                                   # Runs `abricate`.
```
##### 7b. Summarizing AMR Profiles
```bash
nano amr_summary.sh                                                     # Script for summary report
```
```bash
#!/bin/bash

# Define variables
amr_results_dir="amr_results"                                           # Directory containing AMR result files
summary_output_dir="amr_summary"
summary_output_file="amr_summary/amr_summary.csv"                       # Output summary file

# Create summary output folder
mkdir -p $summary_output_dir

# Create header for the summary output file
echo "Gene,Count,Samples" > $summary_output_file

# Confirm the existence of AMR results directory
if [[ ! -d $amr_results_dir ]]; then
    echo "AMR results directory not found!"
    exit 1
else
    echo "AMR results directory exists. Proceed..."
fi

# Summarize AMR profiles from all sample result files
awk 'BEGIN {FS="\t"; OFS=","}
     NR > 1 {count[$6]++; samples[$6] = (samples[$6] ? samples[$6] ";" FILENAME : FILENAME)} 
     END {for (gene in count) print gene, count[gene], samples[gene]}' $amr_results_dir/*_amr_results.tsv >> $summary_output_file

echo "AMR profile summary saved to $summary_output_file"
```
```bash
bash amr_summary.sh
```
#### 8. Detecting Toxin Genes with `Abricate`
Similar to AMR genes detection, the toxin genes in the organism were identified and summarized as follows:
##### 8a. Running `Abricate` to detect toxin genes from the `Virulent Factor Database (VFDB)`

```bash
nano toxin_analysis.sh                                          # Script for toxin analysis
```
```bash
#!/bin/bash

# Define variables
assembly_dir="assembly"                                         # Folder containing assembled genomes.
output_dir="toxin_results"                                      # Output folder.
toxin_db="vfdb"                                                 # Database (Virulent Factor Database) to use for Abricate.

# Create output directory
mkdir -p $output_dir

# Process the contigs.fasta file of each sample and run Abricate to detect toxin genes
for sample_dir in $assembly_dir/*; do
    contigs_file="$sample_dir/contigs.fasta"
    if [[ -f "$contigs_file" ]]; then
        abricate --db $toxin_db --quiet "$contigs_file" > "$output_dir/$(basename $sample_dir)_toxin_results.tsv"
    else
        echo "Warning: contigs.fasta not found in $sample_dir"
    fi
done

echo "Toxin gene analysis completed. Results saved in $output_dir."
```
```bash
bash toxin_analysis.sh
```
##### 8b. Summarize toxin results
```bash
nano toxin_summary.sh                                                 # Toxin summary script
```
```bash
#!/bin/bash

# Define variables
toxin_results_dir="toxin_results"                                     # Directory containing toxin result files
summary_output_dir="toxin_summary"
summary_output_file="toxin_summary/toxin_summary.csv"                 # Output summary file

# Create summary output folder
mkdir -p $summary_output_dir

# Create header for the summary output file
echo "Toxin Gene,Count,Samples" > $summary_output_file

# Check if toxin results directory exists
if [[ ! -d $toxin_results_dir ]]; then
    echo "Toxin results directory not found!"
    exit 1
else
    echo "Toxin results directory exists. Proceed..."
fi

# Summarize toxin genes from all sample result files
awk 'BEGIN {FS="\t"; OFS=","}
     NR > 1 {count[$6]++; samples[$6] = (samples[$6] ? samples[$6] ";" FILENAME : FILENAME)} 
     END {for (gene in count) print gene, count[gene], samples[gene]}' $toxin_results_dir/*_toxin_results.tsv >> $summary_output_file
echo "Toxin gene summary saved to $summary_output_file"
```
```bash
bash toxin_summary.sh
```
### RESULTS




