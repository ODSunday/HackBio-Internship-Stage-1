# HackBio-Internship-Stage-1: Whole Genome Sequencing (WGS)
Whole genome sequencing (WGS) pipelines considered in this stage include reference based and de novo sequence assemblies, applied in three mini projects as:
* Microbes
* Plant
* Human

## PORJECT 1: WHOLE GENOME SEQUENCING ANALYSIS OF THE SOUTH AFRICAN POLONY OUTBREAK
### INTRODUCTION
South Africa faced a serious public health problem in early 2017 due to a foodborne infection traced to polony, a processed cold meats. To confirm the identity and resistance profile of the infectious organism, whole genome sequencing (WGS) analysis becomes highly indispensable. Hence, this current project analyses WGS data from 100 bacterial isolates collected during the 2017â€“2018 South African outbreak, to:  
- Confirm the identity of the organism;
- Determine the antimicrobial resistance (AMR) profile of these pathogens.
- Detect the presence of any toxin accelerating the death rate.
- Suggest possible antibiotics or treatment options for managing the cases.

### METHOD
#### 1a. Downloading the datasets
The script containing the datasets was provided at: `https://raw.githubusercontent.com/HackBio-Internship/2025_project_collection/refs/heads/main/SA_Polony_100_download.sh`. 
The script and the datasets were downloaded and saved properly using the following commands:
```bash
wget https://raw.githubusercontent.com/HackBio-Internship/2025_project_collection/refs/heads/main/SA_Polony_100_download.sh   # Downloads the script provided.

bash SA_Polony_100_download.sh                   # Downloads the data from the script provided.
ls 						                         # Confirms downloaded files.
mkdir raw_data                                   # Creates a new folder for the data.
mv *.fastg.gz raw_data/                          # Moves all the data (ending with the '.fastg.gz' suffix) into to 'raw_data' folder. 

ls -l raw_data | grep ^- | wc -l                 # Confirms the presence and numbers of the files in the 'raw_data' folder.
# ls -l list the files with their details; grep ^- filters regular files only, without folder; wc -l counts number lines, which is equivalent to the number of files in the folder.
```

#### 1b. Downsizing to 50 samples
The 100 samples were downsized to 50, using the following commands:
```bash
mkdir selected_samples                            # Creates a new folder for the randomly selceted data.

for base_name in $(ls raw_data/*_1.fastq.gz | sed 's/_1.fastq.gz//' | shuf -n 50); do   
    cp "${base_name}_1.fastq.gz" selected_samples/                                      
    cp "${base_name}_2.fastq.gz" selected_samples/                                      
done

# ls raw_data/*_1.fastq.gz lists only all forward read files, to forestall duplications;
# sed 's/_1.fastq.gz//' removes the _1.fastq.gz suffix to obtain only the base names; shuf -n 50 randomly selects 50 unique base names.
# The for loop then iterates over each of the selected base name, and
# cp "${base_name}_1.fastq.gz" sample_data/ copies the corresponding forward read of the randomly selected sample into the sample_data folder, while
# cp "${base_name}_2.fastq.gz" sample_data/ copies the corresponding reverse read into the folder.

ls -l selected_samples | grep ^- | wc -l          # Confirms the presence and numbers of the files in the 'selected_samples' folder.
```

#### 2. Quality control with `fastqc`
It is important to check the quality of the data. The quality control was perfomed as follows:
```bash
mkdir qc                                                           # Creates a new folder named 'qc' for the QC reports/outputs.

for sample in selected_samples/*_1.fastq.gz selected_samples/*_2.fastq.gz; do
    fastqc "$sample" -o qc/
done                                                               # The command runs FastQC on all forward and reverse reads of the selected samples and outputs the QC reports to the "qc" folder.

ls -l qc | grep ^- | wc -l                                         # Verifies the number of files in the `qc` folder.

mkdir multiqc_reports                                              # Creates a new folder named 'multiqc_reports' for the aggregated QC reports.

multiqc qc/ -o multiqc_reports/                                    # Aggregates the QC reports and outputs them to the 'multiqc_reports' folder. 
```
The ```html``` report generated by multiqc was downloaded to assess the quality profile of the samples.
The multiqc aggregated the QC reports generated for the forward and reverse reads for each of the 50 randomly-selected samples, now totally 100 samples. This accounted for the subsequent mention of 100 samples, particularly, in this section of the report. 
A large proportion of the selected samples passed the quality assessments. For instance, all 100 samples passed the `Per Sequence Quality Scores`, `Per Base N Content`, and `Adapter Content` assessments, while the `Sequence Quality Histograms` showed that 89 of the 100 samples passed, 4 had warnings, and 7 failed, indicating acceptable quality for most of the sample. 
However, there were few issues of concern in the quality assessment. For example, all 100 samples failed the `Per Base Sequence Content` assessment, while the `Per Sequence GC Content` assessment showed that 98 of the 100 samples had warnings and 2 failed this assessment. Also, the quality assessment indicated some level of duplications, sequence length variability, and overrepresented sequences. 
These suggest potential problems with bais, contamination at specific base positions, library preparations, or sequencing, which may impair the reliability of downstream analyses. Therefore, the samples need trimming and filtering preprocessing to address these quality issues. 
The sample preprocessing steps were performed as shown in the next section. 

### 3. Trimming and filtering with `fastp`
The fastp was used to remove poor-quality bases, short reads or adapter contents, using the following commands:
```bash
mkdir trimmed_samples                                            # Creates a new folder named 'trimmed_samples' for the fastp reports.
mkdir fastp_reports                                              # Creates a new folder named 'fastp_reports' for the html and json outputs from fastp.
nano trim.sh                                                     # Write a script named 'trim.sh' containing the codes that execute fastp.
```
The command in the `trim.sh` script includes:
```bash
#!/bin/bash

for sample in $(ls selected_samples/*_1.fastq.gz | sed 's|.*/||; s/_1.fastq.gz//'); do
    fastp -i "selected_samples/${sample}_1.fastq.gz" \
          -I "selected_samples/${sample}_2.fastq.gz" \
          -o "trimmed_samples/${sample}_1_trimmed.fastq.gz" \
          -O "trimmed_samples/${sample}_2_trimmed.fastq.gz" \
          --html "fastp_reports/${sample}_fastp.html" \
          --json "fastp_reports/${sample}_fastp.json" \
          -q 20 -u 10 -l 50 --trim_front1 5 --trim_front2 5 \
          --cut_mean_quality 20 \
          --cut_front \
          --cut_tail \
          --cut_right \
          --detect_adapter_for_pe
done                                                    

# s|.*/|| leaves only the file name by removing the directory path, and s/_1.fastq.gz// removes the suffix _1.fastq.gz.
# -i is the first input file (forward read); -I is the second input file (reverse read); 
# -o is the output forward read file (after processing); -O is the output reverse read file (after processing).
# --html generats an HTML report of the fastp process into the specified folder.
# -q 20 function sets the the minimum quality score at 20 (trims off bases with a quality score below 20).
# -u 10 filters out reads with a low quality score.
# -l 50 retains only reads longer than 50 bases (discards reads shorter than 50 bases).
# --trim_front1 5 and --trim_front2 5 trims off the first 5 bases from the forward and reverse input files, respectively.
# --cut_mean_quality 20 trims reads according to the average quality score (maintains overall read quality instead of mere individual base quality).
# --detect_adapter_for_pe automatically detects and removes adapter sequences.
```
Running the `fastp` command:
```bash
bash trim.sh                                                   # Performs fastp.
```
Confirming outputs:
```bash
ls -l trimmed_samples | grep ^- | wc -l                        # Confirms the number of files in the 'trimmed_samples' folder.
ls -l fastp_reports | grep ^- | wc -l                          # Confirms the number of files in the 'fastp_reports' folder.
```
The `fastp` reports were aggregated with `multiqc` as follows:
```bash
mkdir new_multiqc_reports                                      # Creates a new folder named 'new_multiqc_reports' for the multiqc reports.
multiqc fastp_reports/ -o new_multiqc_reports/                 # Aggregates the fastp reports and outputs them in the new_multiqc_reports' folder.
```
The aggregated fastp reports were thereafter downloaded and assessed to confirm improved quality.

### 4. De novo assembly with `spades.py`
De novo genome assembly was perform on trimmed reads using SPAdes genome assembler `spades.py` as follow:
```bash
mkdir assembly                                                 # Creates a new folder named 'assembly'.
nano assembler.sh                                              # Writes a bash script for the assembly command as shown below:
```
The command in the `assembler.sh` script:
```bash
#!/bin/bash

for sample in $(ls trimmed_samples/*_1_trimmed.fastq.gz | sed 's|.*/||; s/_1_trimmed.fastq.gz//'); do
    spades.py -1 "trimmed_samples/${sample}_1_trimmed.fastq.gz" \
              -2 "trimmed_samples/${sample}_2_trimmed.fastq.gz" \
              -o "assembly/${sample}_spades" \
	          --threads 4 \
              --phred-offset 33
done

# The ls trimmed_samples/*_1_trimmed.fastq.gz lists all the files with forward read in the trimmed_samples folder, while 
# The sed 's|.*/||; s/_1_trimmed.fastq.gz//' function removes the path and the suffix from the file name to extract the sample name.
# spades.py -1 ... -2 ... -o ... function runs SPAdes for each sample, specifies the paths to the forward and reverse trimmed reads and sets the output folder for each sampleâ€™s assembly.
# --threads 4 uses 4 threads for the assembly process to increase the speed and save runtime a bit.
# --phred-offset 33 enables SPAdes to correctly interpret the quality scores in the fastq files for Phred+33 quality score encoding contained in the samples.
```
Running the `spades.py` command: 
```bash
bash assembler.sh                                               # Performs de novo genome assembly with `Spades.py`.
```
